"""
RAG Engine Module
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Retrieval-Augmented Generation
"""

from typing import List, Dict, Optional, Any
import logging
import json

from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain.chains import LLMChain

# Handle imports for both module and standalone usage
try:
    import config
    from src.vector_store import VectorStoreManager
    from src.document_loader import DocumentLoader
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    import config
    from src.vector_store import VectorStoreManager
    from src.document_loader import DocumentLoader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAGEngine:
    """
    ‡∏Ñ‡∏•‡∏≤‡∏™‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG Pipeline
    ‡∏£‡∏ß‡∏° document retrieval ‡πÅ‡∏•‡∏∞ generation
    """

    def __init__(
        self,
        vector_store_manager: Optional[VectorStoreManager] = None,
        llm_provider: str = None
    ):
        """
        Initialize RAG Engine

        Args:
            vector_store_manager: Vector store manager (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
            llm_provider: LLM provider ("ollama" or "openai")
        """
        self.vector_store = vector_store_manager or VectorStoreManager()
        self.llm_provider = llm_provider or config.LLM_PROVIDER

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á LLM
        self._setup_llm()

    def _setup_llm(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Language Model"""
        try:
            if self.llm_provider == "ollama":
                logger.info(f"Using Ollama model: {config.OLLAMA_MODEL}")
                self.llm = Ollama(
                    model=config.OLLAMA_MODEL,
                    base_url=config.OLLAMA_BASE_URL
                )
            elif self.llm_provider == "openai":
                from langchain_community.llms import OpenAI
                logger.info("Using OpenAI model")
                self.llm = OpenAI(
                    api_key=config.OPENAI_API_KEY,
                    temperature=0.7
                )
            else:
                raise ValueError(f"Unsupported LLM provider: {self.llm_provider}")

            logger.info("LLM initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing LLM: {e}")
            raise

    def retrieve_context(
        self,
        query: str,
        k: int = None
    ) -> List[Document]:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            k: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

        Returns:
            List[Document]: ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        """
        k = k or config.TOP_K_RESULTS
        return self.vector_store.similarity_search(query, k=k)

    def format_context(self, documents: List[Document]) -> str:
        """
        ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô context string

        Args:
            documents: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

        Returns:
            str: context ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß
        """
        if not documents:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"

        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"[‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ {i}]\n{doc.page_content}\n")

        return "\n".join(context_parts)

    def query(
        self,
        question: str,
        k: int = None,
        return_context: bool = False
    ) -> Dict[str, Any]:
        """
        Query RAG system

        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            k: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô context ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            return_context: ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô context ‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

        Returns:
            Dict: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        """
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ context
        retrieved_docs = self.retrieve_context(question, k=k)
        context = self.format_context(retrieved_docs)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° (Control System) ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
{context}

‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
- ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
"""

        # Generate answer
        try:
            answer = self.llm.invoke(prompt)

            result = {
                "question": question,
                "answer": answer,
                "num_sources": len(retrieved_docs)
            }

            if return_context:
                result["context"] = context
                result["sources"] = [
                    {
                        "content": doc.page_content,
                        "metadata": doc.metadata
                    }
                    for doc in retrieved_docs
                ]

            return result

        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return {
                "question": question,
                "answer": f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}",
                "error": True
            }

    def load_knowledge_base(self, directory: str = None):
        """
        ‡πÇ‡∏´‡∏•‡∏î‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå

        Args:
            directory: path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå (default: knowledge_base)
        """
        directory = directory or str(config.KNOWLEDGE_BASE_DIR)

        logger.info(f"Loading knowledge base from {directory}")
        loader = DocumentLoader()
        documents = loader.load_directory(directory)

        if documents:
            self.vector_store.add_documents(documents)
            logger.info(f"Loaded {len(documents)} documents into vector store")
        else:
            logger.warning("No documents found in knowledge base directory")

    def get_status(self) -> Dict[str, Any]:
        """
        ‡∏î‡∏π‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á RAG Engine

        Returns:
            Dict: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        """
        vector_info = self.vector_store.get_collection_info()

        return {
            "llm_provider": self.llm_provider,
            "vector_store": vector_info,
            "embedding_model": self.vector_store.embedding_model_name
        }


def create_rag_engine() -> RAGEngine:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Engine ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÇ‡∏´‡∏•‡∏î‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ

    Returns:
        RAGEngine: RAG engine ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    """
    engine = RAGEngine()
    engine.load_knowledge_base()
    return engine


def main():
    """Test function for standalone usage"""
    print("üß™ Testing RAG Engine...")
    
    try:
        # Test 1: Initialize RAG engine
        vector_store = VectorStoreManager()
        rag_engine = RAGEngine(vector_store_manager=vector_store)
        print("‚úì RAG engine initialized")
        
        # Test 2: Test status
        status = rag_engine.get_status()
        print(f"‚úì LLM Provider: {status['llm_provider']}")
        print(f"‚úì Vector Store: {status['vector_store']['name']}")
        
        # Test 3: Test query
        test_questions = [
            "‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÅ‡∏ö‡∏ö‡∏ß‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£",
            "PID Controller ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á",
            "Transfer Function ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"
        ]
        
        print("\n--- Testing Queries ---")
        for i, question in enumerate(test_questions, 1):
            print(f"\nTest {i}: {question}")
            result = rag_engine.query(question)
            print(f"‚úì Answer: {result['answer'][:100]}...")
            print(f"‚úì Sources: {result['num_sources']}")
        
        print("\n‚úÖ RAG Engine Tests Completed!")
        
    except Exception as e:
        print(f"‚ùå Error in RAG engine test: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

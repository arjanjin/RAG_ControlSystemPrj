"""
Exam Grader Module
‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ RAG
"""

from typing import Dict, List, Any, Optional
import logging
import json

# Handle imports for both module and standalone usage
try:
    import config
    from src.rag_engine import RAGEngine
except ImportError:
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))
    import config
    from src.rag_engine import RAGEngine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ExamGrader:
    """
    ‡∏Ñ‡∏•‡∏≤‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ RAG
    """

    def __init__(self, rag_engine: Optional[RAGEngine] = None):
        """
        Initialize exam grader

        Args:
            rag_engine: RAG Engine (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
        """
        self.rag_engine = rag_engine or RAGEngine()

    def grade_answer(
        self,
        question: str,
        student_answer: str,
        correct_answer: str,
        max_score: int = 100
    ) -> Dict[str, Any]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

        Args:
            question: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            student_answer: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
            correct_answer: ‡πÄ‡∏â‡∏•‡∏¢/‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            max_score: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏° (default: 100)

        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à
        """
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ context ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        context_docs = self.rag_engine.retrieve_context(question)
        context = self.rag_engine.format_context(context_docs)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        prompt = config.GRADING_PROMPT_TEMPLATE.format(
            context=context,
            question=question,
            student_answer=student_answer,
            correct_answer=correct_answer
        )

        try:
            # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏£‡∏ß‡∏à
            response = self.rag_engine.llm.invoke(prompt)

            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° parse JSON response
            result = self._parse_grading_response(response, max_score)
            result['question'] = question
            result['student_answer'] = student_answer
            result['correct_answer'] = correct_answer

            logger.info(f"Graded question - Score: {result['score']}/{max_score}")
            return result

        except Exception as e:
            logger.error(f"Error grading answer: {e}")
            return {
                'question': question,
                'student_answer': student_answer,
                'correct_answer': correct_answer,
                'score': 0,
                'is_correct': False,
                'feedback': f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à: {str(e)}",
                'error': True
            }

    def _parse_grading_response(self, response: str, max_score: int) -> Dict[str, Any]:
        """
        ‡πÅ‡∏õ‡∏•‡∏á response ‡∏à‡∏≤‡∏Å LLM ‡πÄ‡∏õ‡πá‡∏ô dict

        Args:
            response: response ‡∏à‡∏≤‡∏Å LLM
            max_score: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°

        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à
        """
        try:
            # ‡∏•‡∏≠‡∏á parse JSON
            # ‡∏•‡∏ö code blocks ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
            response = response.strip()
            if response.startswith('```'):
                lines = response.split('\n')
                json_start = -1
                json_end = -1
                
                for i, line in enumerate(lines):
                    if line.strip().startswith('{'):
                        json_start = i
                    if line.strip().endswith('}') and json_start != -1:
                        json_end = i
                        break
                
                if json_start != -1 and json_end != -1:
                    response = '\n'.join(lines[json_start:json_end+1])
            
            # ‡∏´‡∏≤ JSON object ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            import re
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                response = json_match.group(0)

            result = json.loads(response)

            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô scale ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            if 'score' in result:
                score = result['score']
                if score > max_score:
                    result['score'] = max_score
                else:
                    result['score'] = int(score)

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            required_fields = ['score', 'is_correct', 'feedback', 'key_points_covered', 'missing_points']
            for field in required_fields:
                if field not in result:
                    if field == 'score':
                        result[field] = 0
                    elif field == 'is_correct':
                        result[field] = False
                    elif field == 'feedback':
                        result[field] = "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ"
                    else:
                        result[field] = []

            return result

        except (json.JSONDecodeError, Exception) as e:
            # ‡∏ñ‡πâ‡∏≤ parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å text
            logger.warning(f"Could not parse JSON: {e}, using fallback parsing")

            score = 0
            is_correct = False
            feedback = response

            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å text
            import re
            score_patterns = [
                r'"score"\s*:\s*(\d+)',
                r'score["\s:]+(\d+)',
                r'‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô[:\s]*(\d+)',
                r'(\d+)\s*/?100',
                r'(\d+)\s*%'
            ]
            
            for pattern in score_patterns:
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    try:
                        score = min(max_score, int(match.group(1)))
                        break
                    except:
                        continue

            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ is_correct
            if any(word in response.lower() for word in ['true', '‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á', 'correct']):
                is_correct = True

            return {
                'score': score,
                'is_correct': is_correct,
                'feedback': feedback,
                'key_points_covered': [],
                'missing_points': []
            }

    def grade_exam(
        self,
        exam_data: List[Dict[str, str]],
        max_score_per_question: int = 100
    ) -> Dict[str, Any]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∏‡∏î

        Args:
            exam_data: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö [{'question': ..., 'student_answer': ..., 'correct_answer': ...}]
            max_score_per_question: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ï‡πá‡∏°‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠

        Returns:
            Dict: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        results = []
        total_score = 0
        max_total_score = len(exam_data) * max_score_per_question

        logger.info(f"Grading exam with {len(exam_data)} questions")

        for i, item in enumerate(exam_data, 1):
            logger.info(f"Grading question {i}/{len(exam_data)}")

            result = self.grade_answer(
                question=item.get('question', ''),
                student_answer=item.get('student_answer', ''),
                correct_answer=item.get('correct_answer', ''),
                max_score=max_score_per_question
            )

            result['question_number'] = i
            results.append(result)
            total_score += result.get('score', 0)

        percentage = (total_score / max_total_score * 100) if max_total_score > 0 else 0

        return {
            'results': results,
            'total_score': total_score,
            'max_score': max_total_score,
            'percentage': round(percentage, 2),
            'num_questions': len(exam_data),
            'summary': self._generate_summary(results)
        }

    def _generate_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à

        Args:
            results: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à

        Returns:
            Dict: ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        """
        num_correct = sum(1 for r in results if r.get('is_correct', False))
        num_questions = len(results)
        avg_score = sum(r.get('score', 0) for r in results) / num_questions if num_questions > 0 else 0

        return {
            'num_correct': num_correct,
            'num_incorrect': num_questions - num_correct,
            'average_score': round(avg_score, 2),
            'pass': avg_score >= 50  # ‡∏ú‡πà‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50
        }

    def generate_feedback_report(self, grading_results: Dict[str, Any]) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô feedback ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

        Args:
            grading_results: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å grade_exam

        Returns:
            str: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô feedback
        """
        report = []
        report.append("=" * 60)
        report.append("‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ß‡∏¥‡∏ä‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°")
        report.append("=" * 60)
        report.append("")

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        report.append(f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°: {grading_results['total_score']}/{grading_results['max_score']}")
        report.append(f"‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå: {grading_results['percentage']}%")
        report.append(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å: {grading_results['summary']['num_correct']}/{grading_results['num_questions']}")
        report.append(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö: {'‡∏ú‡πà‡∏≤‡∏ô' if grading_results['summary']['pass'] else '‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô'}")
        report.append("")
        report.append("=" * 60)
        report.append("‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠")
        report.append("=" * 60)
        report.append("")

        # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠
        for result in grading_results['results']:
            report.append(f"‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {result['question_number']}")
            report.append(f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {result['question']}")
            report.append(f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {result['score']}")
            report.append(f"‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {'‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á' if result.get('is_correct') else '‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á'}")
            report.append(f"\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤:\n{result['student_answer']}")
            report.append(f"\n‡πÄ‡∏â‡∏•‡∏¢:\n{result['correct_answer']}")
            report.append(f"\n‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n{result['feedback']}")

            if result.get('key_points_covered'):
                report.append(f"\n‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å:")
                for point in result['key_points_covered']:
                    report.append(f"  - {point}")

            if result.get('missing_points'):
                report.append(f"\n‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:")
                for point in result['missing_points']:
                    report.append(f"  - {point}")

            report.append("\n" + "-" * 60 + "\n")

        return "\n".join(report)


def create_exam_grader() -> ExamGrader:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á ExamGrader ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

    Returns:
        ExamGrader: exam grader ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    """
    rag_engine = RAGEngine()
    rag_engine.load_knowledge_base()
    return ExamGrader(rag_engine)


def main():
    """Test function for standalone usage"""
    print("üß™ Testing Exam Grader...")
    
    try:
        # Test 1: Initialize exam grader
        from src.vector_store import VectorStoreManager
        
        vector_store = VectorStoreManager()
        rag_engine = RAGEngine(vector_store_manager=vector_store)
        grader = ExamGrader(rag_engine)
        print("‚úì Exam grader initialized")
        
        # Test 2: Test individual answer grading
        test_case = {
            'question': '‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÅ‡∏ö‡∏ö‡∏ß‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£',
            'student_answer': '‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ feedback',
            'correct_answer': '‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏î‡πâ'
        }
        
        print("\n--- Testing Individual Answer Grading ---")
        print(f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {test_case['question']}")
        print(f"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤: {test_case['student_answer']}")
        
        result = grader.grade_answer(**test_case)
        print(f"‚úì ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {result['score']}/100")
        print(f"‚úì ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {result['is_correct']}")
        print(f"‚úì Feedback: {result['feedback'][:100]}...")
        
        # Test 3: Test JSON parsing with various formats
        print("\n--- Testing JSON Parsing ---")
        test_responses = [
            '{"score": 85, "is_correct": true, "feedback": "‡∏î‡∏µ"}',
            '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 75 ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: false',
            'Some text before {"score": 90} and after',
        ]
        
        for i, response in enumerate(test_responses, 1):
            parsed = grader._parse_grading_response(response, 100)
            print(f"‚úì Test {i}: Score = {parsed['score']}")
        
        print("\n‚úÖ Exam Grader Tests Completed!")
        
    except Exception as e:
        print(f"‚ùå Error in exam grader test: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
